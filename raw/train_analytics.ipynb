{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# 1. Introdução e Metadados\n",
        "\n",
        "### 1.1. Contexto e Fontes de Dados\n",
        "\n",
        "Os dados foram obtidos a partir da plataforma [Kaggle](https://www.kaggle.com/), em uma competição de predição de malware da Microsoft.\n",
        "\n",
        "- **Fonte Principal:** [Microsoft Malware Prediction](https://www.kaggle.com/competitions/microsoft-malware-prediction/data).\n",
        "\n",
        "- **Fonte Original:** Os dados são derivados de telemetria de máquinas Windows, coletados pela Microsoft para identificar padrões de malware.\n",
        "\n",
        "### Sobre a Competição\n",
        "\n",
        "A competição Microsoft Malware Prediction tem como objetivo prever se uma máquina será infectada por malware com base em características do sistema operacional, hardware e software instalado. Esta é uma tarefa de classificação binária onde o target é a variável `HasDetections`.\n",
        "\n",
        "### 1.2. Sobre o Dataset e Estrutura\n",
        "\n",
        "O dataset descreve características de máquinas Windows e sua propensão a serem infectadas por malware. As informações abrangem várias áreas principais:\n",
        "\n",
        "- **Características do Sistema:** Informações sobre o sistema operacional, versão, arquitetura e configurações.\n",
        "- **Hardware:** Dados sobre processador, memória RAM, espaço em disco e outros componentes.\n",
        "- **Software:** Informações sobre programas instalados, versões e configurações.\n",
        "- **Telemetria:** Dados de uso e comportamento do sistema.\n",
        "\n",
        "A escolha deste conjunto de dados se deve à sua relevância para segurança cibernética e ao desafio de classificação binária que apresenta. Este dataset combina um grande volume de registros com alta dimensionalidade, oferecendo um cenário realista para projetos de Machine Learning em segurança.\n",
        "\n",
        "### 1.3. Desafios de Qualidade e Limitações Iniciais\n",
        "\n",
        "Entre as oportunidades de tratamento já identificadas, destacam-se:\n",
        "- **Alta Dimensionalidade:** O dataset possui um grande número de features, muitas das quais podem ser irrelevantes ou redundantes.\n",
        "- **Dados Ausentes:** Presença significativa de valores faltantes que precisarão de estratégias de tratamento adequadas.\n",
        "- **Desbalanceamento de Classes:** Possível desbalanceamento entre classes de malware e não-malware.\n",
        "- **Engenharia de Features:** Necessidade de criar novas features ou transformar as existentes para melhorar a performance do modelo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Análise Exploratória\n",
        "\n",
        "A seguir, utilizaremos Python para auxiliar na análise dos dados brutos. Com a biblioteca [`Pandas`](https://pandas.pydata.org/), podemos visualizar e alterar os dados utilizando `Dataframes` e `Series`. Uma `Series` é um array unidimensional, e um `Dataframe` é como uma tabela, em que cada coluna é uma `Series`.  \n",
        "\n",
        "Além dessas bibliotecas, também podemos usar a `seaborn` para desenhar gráficos com base nos dados das `Series` e `Dataframes`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Importação das bibliotecas e carregamento do Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações para melhor visualização\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carregamento do dataset com otimizações para arquivos grandes\n",
        "df = pd.read_csv(\"./train.csv\", sep=\",\", encoding='utf8', low_memory=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Head do Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A tabela apresentada mostra as cinco primeiras entradas do nosso conjunto de dados, o que nos permite uma verificação inicial das colunas disponíveis e dos tipos de dados que elas contêm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Dimensões do Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Após a primeira visualização, o próximo passo é verificar as dimensões do nosso conjunto de dados. A \"dimensão\" de um dataset refere-se à sua estrutura em termos de quantidade de linhas e colunas.\n",
        "\n",
        "Conforme o resultado abaixo, nosso dataset é composto por um grande número de linhas e colunas, o que nos dá uma noção do volume de dados que iremos analisar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "linhas, colunas = df.shape\n",
        "\n",
        "print(f\"Número de tuplas: {linhas:,}\")\n",
        "print(f\"Número de colunas: {colunas}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.4 Identificação das Colunas e Tipos de Dados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora que conhecemos as dimensões gerais do nosso dataset, o próximo passo é nos aprofundarmos nessas colunas. Precisamos identificar o nome de cada uma das variáveis que temos à disposição para entender com quais informações estamos trabalhando.\n",
        "\n",
        "O comando a seguir listará todas as colunas, dando uma visão dos atributos disponíveis para cada máquina.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total de colunas: {len(df.columns)}\")\n",
        "print(\"\\nPrimeiras 20 colunas:\")\n",
        "for i, col in enumerate(df.columns[:20]):\n",
        "    print(f\"{i+1:2d}. {col}\")\n",
        "\n",
        "if len(df.columns) > 20:\n",
        "    print(f\"\\n... e mais {len(df.columns) - 20} colunas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Com os nomes das colunas já identificados, vamos entender como os dados de cada variável estão sendo interpretados. Verificar os tipos de dados (dtypes) é um passo importante para garantir a integridade da análise e planejar a etapa de limpeza.\n",
        "\n",
        "O resultado a seguir nos revela a distribuição dos tipos de dados no dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise dos tipos de dados\n",
        "dtype_counts = df.dtypes.value_counts()\n",
        "print(\"Distribuição dos tipos de dados:\")\n",
        "print(dtype_counts)\n",
        "\n",
        "print(\"\\nDetalhamento por tipo:\")\n",
        "for dtype in df.dtypes.unique():\n",
        "    cols = df.select_dtypes(include=[dtype]).columns.tolist()\n",
        "    print(f\"\\n{dtype}: {len(cols)} colunas\")\n",
        "    if len(cols) <= 10:\n",
        "        print(f\"  {cols}\")\n",
        "    else:\n",
        "        print(f\"  {cols[:10]} ... e mais {len(cols)-10} colunas\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Qualidade dos Dados\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5.1 Análise de Valores Ausentes (Nulos/NaN)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dados não nulos\n",
        "\n",
        "Enquanto o passo anterior nos mostrou apenas os tipos de dados, a análise a seguir também revelará a quantidade de valores não nulos em cada coluna, nos dando um panorama sobre a presença de dados ausentes.\n",
        "\n",
        "Este diagnóstico serve para identificar lacunas que precisam ser tratadas antes de qualquer análise mais profunda.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informações gerais sobre o dataset\n",
        "print(\"Informações gerais do dataset:\")\n",
        "print(f\"Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(f\"\\nResumo de informações:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dados nulos\n",
        "\n",
        "No passo anterior, o método `info()` nos deu um bom indicativo da presença de dados ausentes. Para abordar esse problema, vamos calcular a soma exata de valores nulos para cada coluna do dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise de valores nulos\n",
        "null_counts = df.isnull().sum()\n",
        "null_percentages = (null_counts / len(df)) * 100\n",
        "\n",
        "# Criar DataFrame com informações de nulos\n",
        "null_info = pd.DataFrame({\n",
        "    'Coluna': null_counts.index,\n",
        "    'Valores_Nulos': null_counts.values,\n",
        "    'Percentual_Nulos': null_percentages.values\n",
        "})\n",
        "\n",
        "# Ordenar por percentual de nulos\n",
        "null_info = null_info.sort_values('Percentual_Nulos', ascending=False)\n",
        "\n",
        "print(\"Top 20 colunas com mais valores nulos:\")\n",
        "print(null_info.head(20).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5.2 Análise da Variável Target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para um problema de classificação binária, é fundamental entender a distribuição da variável target. Vamos analisar a variável `HasDetections` que indica se a máquina foi infectada por malware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise da variável target\n",
        "if 'HasDetections' in df.columns:\n",
        "    target_counts = df['HasDetections'].value_counts()\n",
        "    target_percentages = df['HasDetections'].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(\"Distribuição da variável target (HasDetections):\")\n",
        "    print(f\"\\nContagem:\")\n",
        "    for value, count in target_counts.items():\n",
        "        print(f\"  {value}: {count:,} ({target_percentages[value]:.2f}%)\")\n",
        "    \n",
        "    # Visualização da distribuição\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    ax = sns.countplot(data=df, x='HasDetections', palette=['#5bc0de', '#d9534f'])\n",
        "    ax.set_title('Distribuição da Variável Target (HasDetections)')\n",
        "    ax.set_xlabel('HasDetections (0=Sem Malware, 1=Com Malware)')\n",
        "    ax.set_ylabel('Contagem')\n",
        "    \n",
        "    # Adicionar percentuais nas barras\n",
        "    for i, p in enumerate(ax.patches):\n",
        "        height = p.get_height()\n",
        "        ax.text(p.get_x() + p.get_width()/2., height + height*0.01,\n",
        "                f'{height:,}\\n({target_percentages[i]:.1f}%)',\n",
        "                ha='center', va='bottom')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Verificar balanceamento\n",
        "    balance_ratio = min(target_counts) / max(target_counts)\n",
        "    print(f\"\\nRazão de balanceamento: {balance_ratio:.3f}\")\n",
        "    if balance_ratio < 0.1:\n",
        "        print(\"⚠️  Dataset altamente desbalanceado!\")\n",
        "    elif balance_ratio < 0.3:\n",
        "        print(\"⚠️  Dataset moderadamente desbalanceado\")\n",
        "    else:\n",
        "        print(\"✅ Dataset relativamente balanceado\")\n",
        "        \n",
        "else:\n",
        "    print(\"Variável 'HasDetections' não encontrada no dataset.\")\n",
        "    print(\"Colunas disponíveis que podem ser targets:\")\n",
        "    potential_targets = [col for col in df.columns if 'detect' in col.lower() or 'target' in col.lower() or 'label' in col.lower()]\n",
        "    if potential_targets:\n",
        "        print(potential_targets)\n",
        "    else:\n",
        "        print(\"Nenhuma coluna target óbvia encontrada.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusão da Análise Exploratória e Próximos Passos\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A fase de Análise Exploratória de Dados foi concluída, nos fornecendo um panorama sobre a estrutura, a qualidade e as características do nosso conjunto de dados. Através desta investigação, embora rico em informações, o dataset possui diversas características que precisam ser consideradas antes de qualquer análise aprofundada.\n",
        "\n",
        "Os principais achados identificados foram:\n",
        "\n",
        "- **Dimensões do Dataset:** Um dataset de grande escala com muitas observações e features.\n",
        "- **Dados Ausentes:** Presença de valores faltantes que precisarão de estratégias de tratamento.\n",
        "- **Dados Duplicados:** Verificação da existência de registros duplicados.\n",
        "- **Variável Target:** Análise da distribuição da variável `HasDetections` e verificação de balanceamento.\n",
        "- **Correlações:** Identificação de variáveis altamente correlacionadas que podem ser redundantes.\n",
        "- **Variabilidade:** Análise de valores únicos para identificar variáveis constantes ou com baixa variabilidade.\n",
        "\n",
        "A conclusão desta etapa é que o dataset em seu estado bruto apresenta características típicas de dados de telemetria de sistemas, com alta dimensionalidade e complexidade. Portanto, o próximo passo será focado na **Engenharia de Features** e **Limpeza de Dados**, onde iremos executar um plano para otimizar o dataset para modelagem de Machine Learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
