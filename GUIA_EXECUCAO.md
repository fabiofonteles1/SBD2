# ðŸš€ GUIA DE EXECUÃ‡ÃƒO - DATA LAKEHOUSE SBD-2

## ðŸ“‹ VISÃƒO GERAL

Este guia apresenta o passo a passo completo para executar o projeto Data Lakehouse, desde a configuraÃ§Ã£o inicial atÃ© a validaÃ§Ã£o final do sistema.

---

## ðŸŽ¯ OBJETIVOS DO PROJETO

O projeto implementa um Data Lakehouse com arquitetura em camadas:
- **Bronze (Raw)**: Dados brutos sem tratamento
- **Silver (Processed)**: Dados limpos e estruturados
- **Gold (Analytics)**: Dados agregados para anÃ¡lise

---

## ðŸ“‹ PRÃ‰-REQUISITOS

### Software NecessÃ¡rio
- **Python 3.8+** (recomendado 3.9 ou superior)
- **Docker Desktop** (versÃ£o 20.10+)
- **Git** (para controle de versÃ£o)
- **8GB RAM** (mÃ­nimo recomendado)
- **10GB espaÃ§o em disco** (para dados e containers)

### VerificaÃ§Ã£o dos PrÃ©-requisitos
```bash
# Verificar Python
python --version

# Verificar Docker
docker --version

# Verificar Git
git --version
```

---

## ðŸ› ï¸ CONFIGURAÃ‡ÃƒO INICIAL

### Passo 1: Navegar para o DiretÃ³rio do Projeto
```bash
cd "C:\Users\fabio\OneDrive\Ãrea de Trabalho\sbd-2"
```

### Passo 2: Verificar Estrutura do Projeto
```bash
# Listar arquivos e pastas
dir

# Verificar se o arquivo de dados existe
dir raw\train.csv
```

**Estrutura esperada:**
```
sbd-2/
â”œâ”€â”€ docker/
â”œâ”€â”€ docs/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ train.csv (2.4GB)
â”œâ”€â”€ scripts/
â”œâ”€â”€ README.md
â”œâ”€â”€ setup.py
â””â”€â”€ run_all.py
```

---

## ðŸš€ EXECUÃ‡ÃƒO AUTOMÃTICA (RECOMENDADO)

### OpÃ§Ã£o 1: ExecuÃ§Ã£o Completa em Uma Etapa
```bash
# Executa setup, validaÃ§Ã£o e automaÃ§Ã£o
python run_all.py
```

**O que este comando faz:**
1. âœ… Verifica prÃ©-requisitos
2. âœ… Instala dependÃªncias Python
3. âœ… Cria estrutura de diretÃ³rios
4. âœ… Configura repositÃ³rio Git
5. âœ… Valida configuraÃ§Ã£o
6. âœ… Inicia containers Docker
7. âœ… Executa ETL completo
8. âœ… Valida lakehouse

---

## ðŸ”§ EXECUÃ‡ÃƒO MANUAL (PASSO A PASSO)

### Etapa 1: Setup do Projeto
```bash
# Executa configuraÃ§Ã£o inicial
python setup.py
```

**O que acontece:**
- Verifica Python e Docker
- Instala dependÃªncias (`pandas`, `psycopg2`, `sqlalchemy`, etc.)
- Cria diretÃ³rios necessÃ¡rios (`silver/`, `gold/`, `logs/`, `output/`)
- Configura repositÃ³rio Git
- Faz commit inicial

**SaÃ­da esperada:**
```
=== SETUP DO PROJETO DATA LAKEHOUSE ===
âœ“ Python 3.9 encontrado
âœ“ Docker 20.10 disponÃ­vel
âœ“ DependÃªncias instaladas
âœ“ DiretÃ³rios criados
âœ“ Git configurado
=== SETUP CONCLUÃDO ===
```

### Etapa 2: ValidaÃ§Ã£o do Setup
```bash
# Valida se tudo estÃ¡ configurado corretamente
python scripts/validate_setup.py
```

**O que Ã© verificado:**
- âœ… Estrutura de arquivos
- âœ… Docker funcionando
- âœ… Containers rodando
- âœ… ConexÃ£o com banco de dados
- âœ… Tabelas Silver criadas
- âœ… Arquivo de dados presente
- âœ… Pacotes Python instalados

**SaÃ­da esperada:**
```
=== INICIANDO VALIDAÃ‡ÃƒO DO SETUP ===
âœ“ Estrutura de arquivos OK
âœ“ Docker disponÃ­vel
âœ“ Container PostgreSQL rodando
âœ“ ConexÃ£o com banco de dados OK
âœ“ Tabelas Silver OK: ['train_data', 'data_metadata', 'processing_logs']
âœ“ Arquivo de dados OK: 2.40 GB
âœ“ Pacotes Python OK
ðŸŽ‰ SETUP VALIDADO COM SUCESSO!
```

### Etapa 3: AutomaÃ§Ã£o Completa
```bash
# Executa ETL e popula o lakehouse
python scripts/run_automation.py
```

**Processo detalhado:**

#### 3.1 InicializaÃ§Ã£o do Docker
```bash
# Para containers existentes
docker-compose -f docker/docker-compose.yml down

# Inicia serviÃ§os
docker-compose -f docker/docker-compose.yml up -d
```

**Containers criados:**
- `sbd2_postgres`: Banco PostgreSQL 15
- `sbd2_pgadmin`: Interface web pgAdmin

#### 3.2 CriaÃ§Ã£o do Banco de Dados
- Executa script `docker/init.sql`
- Cria schema `silver`
- Cria tabelas: `train_data`, `data_metadata`, `processing_logs`
- Configura Ã­ndices e constraints

#### 3.3 Processamento ETL
- LÃª arquivo `raw/train.csv` (2.4GB)
- Processa em chunks para otimizar memÃ³ria
- Aplica transformaÃ§Ãµes de limpeza
- Insere dados no PostgreSQL
- Registra logs de processamento

**SaÃ­da esperada:**
```
=== INICIANDO AUTOMAÃ‡ÃƒO COMPLETA ===
âœ“ Docker verificado
âœ“ Containers iniciados
âœ“ Banco de dados disponÃ­vel
âœ“ ETL executado com sucesso
âœ“ Lakehouse validado
=== AUTOMAÃ‡ÃƒO CONCLUÃDA COM SUCESSO ===
```

---

## ðŸ“Š ACESSO AOS RECURSOS

### 1. Jupyter Notebooks
```bash
# Iniciar Jupyter
jupyter notebook notebooks/

# Ou usar VS Code
code notebooks/
```

**Notebooks disponÃ­veis:**
- `01_data_exploration.ipynb`: AnÃ¡lise dos dados brutos
- `02_etl_raw_to_silver.ipynb`: Processo ETL
- `03_lakehouse_validation.ipynb`: ValidaÃ§Ã£o final

### 2. pgAdmin (Interface Web)
- **URL**: http://localhost:8080
- **Email**: admin@sbd2.com
- **Senha**: admin123

**ConfiguraÃ§Ã£o do servidor:**
- **Host**: postgres (ou localhost)
- **Port**: 5432
- **Database**: sbd2_lakehouse
- **Username**: sbd2_user
- **Password**: sbd2_password

### 3. ConexÃ£o Direta ao Banco
```python
import psycopg2

conn = psycopg2.connect(
    host='localhost',
    port=5432,
    database='sbd2_lakehouse',
    user='sbd2_user',
    password='sbd2_password'
)
```

---

## ðŸ” MONITORAMENTO E LOGS

### Logs de ExecuÃ§Ã£o
```bash
# Logs de automaÃ§Ã£o
type automation.log

# Logs de ETL
type etl.log

# Logs de validaÃ§Ã£o
type validation_report.txt
```

### Logs do Docker
```bash
# Logs do PostgreSQL
docker logs sbd2_postgres

# Logs do pgAdmin
docker logs sbd2_pgadmin

# Status dos containers
docker ps
```

### MÃ©tricas de Performance
- **Registros processados**: Verificar em `silver.processing_logs`
- **Tempo de execuÃ§Ã£o**: Logs de inÃ­cio/fim
- **Qualidade dos dados**: EstatÃ­sticas no banco
- **Uso de recursos**: Monitor do sistema

---

## ðŸ› ï¸ TROUBLESHOOTING

### Problema 1: Docker nÃ£o inicia
**Sintomas:**
```
Error: Docker not running
```

**SoluÃ§Ã£o:**
1. Verificar se Docker Desktop estÃ¡ rodando
2. Reiniciar Docker Desktop
3. Verificar recursos disponÃ­veis (RAM/CPU)

### Problema 2: Erro de conexÃ£o com banco
**Sintomas:**
```
psycopg2.OperationalError: connection refused
```

**SoluÃ§Ã£o:**
1. Verificar se container estÃ¡ rodando: `docker ps`
2. Aguardar inicializaÃ§Ã£o: `docker logs sbd2_postgres`
3. Verificar porta 5432: `netstat -an | findstr 5432`

### Problema 3: Arquivo de dados nÃ£o encontrado
**Sintomas:**
```
FileNotFoundError: raw/train.csv not found
```

**SoluÃ§Ã£o:**
1. Verificar se arquivo existe: `dir raw\train.csv`
2. Verificar permissÃµes de leitura
3. Verificar espaÃ§o em disco

### Problema 4: DependÃªncias Python nÃ£o instaladas
**Sintomas:**
```
ModuleNotFoundError: No module named 'pandas'
```

**SoluÃ§Ã£o:**
```bash
# Instalar dependÃªncias
pip install -r scripts/requirements.txt

# Ou instalar individualmente
pip install pandas psycopg2-binary sqlalchemy
```

---

## ðŸ“ˆ VALIDAÃ‡ÃƒO DO SUCESSO

### 1. Verificar Containers
```bash
docker ps
```
**SaÃ­da esperada:**
```
CONTAINER ID   IMAGE                STATUS
abc123def456   postgres:15          Up 2 hours
def456ghi789   dpage/pgadmin4      Up 2 hours
```

### 2. Verificar Banco de Dados
```sql
-- Conectar ao banco e executar:
SELECT table_name FROM information_schema.tables WHERE table_schema = 'silver';
```
**Resultado esperado:**
```
train_data
data_metadata
processing_logs
```

### 3. Verificar Logs de Processamento
```sql
SELECT process_name, status, records_processed, completed_at 
FROM silver.processing_logs 
ORDER BY completed_at DESC;
```

### 4. Verificar Dados Processados
```sql
SELECT COUNT(*) as total_records FROM silver.train_data;
```

---

## ðŸ”„ COMANDOS ÃšTEIS

### Gerenciamento de Containers
```bash
# Parar todos os containers
docker-compose -f docker/docker-compose.yml down

# Iniciar containers
docker-compose -f docker/docker-compose.yml up -d

# Reiniciar containers
docker-compose -f docker/docker-compose.yml restart

# Ver logs em tempo real
docker-compose -f docker/docker-compose.yml logs -f
```

### Limpeza do Sistema
```bash
# Parar e remover containers
docker-compose -f docker/docker-compose.yml down -v

# Limpar volumes Docker
docker volume prune

# Limpar logs
del automation.log etl.log validation_report.txt
```

### Backup e RestauraÃ§Ã£o
```bash
# Backup do banco
docker exec sbd2_postgres pg_dump -U sbd2_user sbd2_lakehouse > backup.sql

# Restaurar banco
docker exec -i sbd2_postgres psql -U sbd2_user sbd2_lakehouse < backup.sql
```

---

## ðŸ“š PRÃ“XIMOS PASSOS

### 1. AnÃ¡lise dos Dados
- Execute os notebooks em `notebooks/`
- Explore os dados com visualizaÃ§Ãµes
- Identifique padrÃµes e insights

### 2. ExpansÃ£o do Sistema
- Adicione mais fontes de dados
- Implemente camada Gold
- Crie dashboards interativos

### 3. OtimizaÃ§Ã£o
- Ajuste parÃ¢metros de performance
- Implemente cache
- Configure monitoramento avanÃ§ado

### 4. DocumentaÃ§Ã£o
- Atualize dicionÃ¡rios de dados
- Documente regras de negÃ³cio
- Crie guias de usuÃ¡rio

---

## ðŸ†˜ SUPORTE

### Em Caso de Problemas
1. **Verificar logs**: `automation.log`, `etl.log`
2. **Validar setup**: `python scripts/validate_setup.py`
3. **Reiniciar sistema**: `python run_all.py`
4. **Consultar documentaÃ§Ã£o**: `docs/README.md`

### Contatos
- **Equipe**: SBD-2
- **Data**: 10/10/2025
- **VersÃ£o**: 1.0.0

---

## ðŸŽ‰ CONCLUSÃƒO

Este guia fornece todas as informaÃ§Ãµes necessÃ¡rias para executar o projeto Data Lakehouse SBD-2 com sucesso. O sistema estÃ¡ projetado para ser robusto, escalÃ¡vel e fÃ¡cil de usar.

**Lembre-se**: Sempre verifique os logs e monitore o sistema durante a execuÃ§Ã£o para garantir que tudo estÃ¡ funcionando corretamente.

---

*Data Lakehouse SBD-2 - Transformando dados brutos em insights valiosos! ðŸš€*
