{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lise de Dados - Microsoft Security Incident Prediction (Camada Silver)\n",
        "\n",
        "Este notebook realiza an√°lises explorat√≥rias e estat√≠sticas dos dados processados da camada Silver do dataset Microsoft Security Incident Prediction.\n",
        "\n",
        "## Objetivos da An√°lise\n",
        "- Compreender a distribui√ß√£o dos dados\n",
        "- Identificar padr√µes e tend√™ncias\n",
        "- Analisar a qualidade dos dados processados\n",
        "- Preparar insights para modelagem de Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importa√ß√£o das Bibliotecas e Carregamento dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configura√ß√µes para visualiza√ß√£o\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Carregamento dos dados processados da camada Silver\n",
        "df = pd.read_csv('security_incident_prediction_silver.csv', low_memory=False)\n",
        "\n",
        "print(\"Dataset carregado com sucesso!\")\n",
        "print(f\"Dimens√µes do dataset: {df.shape}\")\n",
        "print(f\"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Visualizar as primeiras linhas\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. An√°lise Geral dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informa√ß√µes gerais sobre o dataset\n",
        "print(\"=== INFORMA√á√ïES GERAIS DO DATASET ===\")\n",
        "print(f\"N√∫mero de registros: {len(df):,}\")\n",
        "print(f\"N√∫mero de colunas: {len(df.columns)}\")\n",
        "print(f\"Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "print(\"\\n=== TIPOS DE DADOS ===\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(\"\\n=== INFORMA√á√ïES DETALHADAS ===\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n=== ESTAT√çSTICAS DESCRITIVAS ===\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lise da Vari√°vel Target (IncidentGrade)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise da vari√°vel target IncidentGrade\n",
        "if 'incidentgrade' in df.columns:\n",
        "    target_counts = df['incidentgrade'].value_counts()\n",
        "    target_percentages = df['incidentgrade'].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(\"=== DISTRIBUI√á√ÉO DA VARI√ÅVEL TARGET (IncidentGrade) ===\")\n",
        "    print(\"\\nContagem por classe:\")\n",
        "    for value, count in target_counts.items():\n",
        "        print(f\"  Classe {value}: {count:,} ({target_percentages[value]:.2f}%)\")\n",
        "    \n",
        "    # Mapear os valores para nomes mais leg√≠veis (baseado no dataset original)\n",
        "    class_mapping = {0: 'FalsePositive', 1: 'TruePositive', 2: 'BenignPositive'}\n",
        "    print(\"\\nMapeamento das classes:\")\n",
        "    for encoded_val, original_val in class_mapping.items():\n",
        "        if encoded_val in target_counts.index:\n",
        "            count = target_counts[encoded_val]\n",
        "            percentage = target_percentages[encoded_val]\n",
        "            print(f\"  {encoded_val} = {original_val}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualiza√ß√£o da distribui√ß√£o\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Subplot 1: Gr√°fico de barras\n",
        "    plt.subplot(1, 2, 1)\n",
        "    bars = plt.bar(target_counts.index, target_counts.values, color=['#ff7f7f', '#7fbf7f', '#7f7fff'])\n",
        "    plt.title('Distribui√ß√£o da Vari√°vel Target (IncidentGrade)')\n",
        "    plt.xlabel('Classe')\n",
        "    plt.ylabel('Contagem')\n",
        "    \n",
        "    # Adicionar percentuais nas barras\n",
        "    for i, (bar, count, pct) in enumerate(zip(bars, target_counts.values, target_percentages.values)):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.01,\n",
        "                f'{count:,}\\n({pct:.1f}%)', ha='center', va='bottom')\n",
        "    \n",
        "    # Subplot 2: Gr√°fico de pizza\n",
        "    plt.subplot(1, 2, 2)\n",
        "    labels = [f'{class_mapping.get(i, f\"Classe {i}\")}\\n({pct:.1f}%)' \n",
        "              for i, pct in zip(target_counts.index, target_percentages.values)]\n",
        "    plt.pie(target_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title('Distribui√ß√£o Percentual das Classes')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # An√°lise de balanceamento\n",
        "    balance_ratio = min(target_counts) / max(target_counts)\n",
        "    print(f\"\\nRaz√£o de balanceamento: {balance_ratio:.3f}\")\n",
        "    if balance_ratio < 0.1:\n",
        "        print(\"  Dataset altamente desbalanceado!\")\n",
        "    elif balance_ratio < 0.3:\n",
        "        print(\"  Dataset moderadamente desbalanceado\")\n",
        "    else:\n",
        "        print(\" Dataset relativamente balanceado\")\n",
        "        \n",
        "else:\n",
        "    print(\"Vari√°vel 'incidentgrade' n√£o encontrada no dataset.\")\n",
        "    print(\"Colunas dispon√≠veis:\", df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. An√°lise de Categorias de Incidentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise das categorias de incidentes\n",
        "if 'category' in df.columns:\n",
        "    category_counts = df['category'].value_counts()\n",
        "    category_percentages = df['category'].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(\"=== AN√ÅLISE DE CATEGORIAS DE INCIDENTES ===\")\n",
        "    print(f\"Total de categorias √∫nicas: {len(category_counts)}\")\n",
        "    \n",
        "    print(\"\\nTop 10 categorias mais frequentes:\")\n",
        "    for i, (category, count) in enumerate(category_counts.head(10).items()):\n",
        "        percentage = category_percentages[category]\n",
        "        print(f\"  {i+1:2d}. Categoria {category}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualiza√ß√£o das categorias\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    # Gr√°fico de barras horizontais para as top categorias\n",
        "    top_categories = category_counts.head(15)\n",
        "    plt.barh(range(len(top_categories)), top_categories.values, color='skyblue')\n",
        "    plt.yticks(range(len(top_categories)), [f'Categoria {cat}' for cat in top_categories.index])\n",
        "    plt.xlabel('N√∫mero de Incidentes')\n",
        "    plt.title('Top 15 Categorias de Incidentes por Frequ√™ncia')\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # Adicionar valores nas barras\n",
        "    for i, (cat, count) in enumerate(top_categories.items()):\n",
        "        plt.text(count + count*0.01, i, f'{count:,}', va='center', ha='left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # An√°lise de distribui√ß√£o por categoria e incident grade\n",
        "    if 'incidentgrade' in df.columns:\n",
        "        print(\"\\n=== DISTRIBUI√á√ÉO DE CATEGORIAS POR TIPO DE INCIDENTE ===\")\n",
        "        cross_tab = pd.crosstab(df['category'], df['incidentgrade'], normalize='index') * 100\n",
        "        \n",
        "        # Mostrar as top 5 categorias\n",
        "        top_5_categories = category_counts.head(5).index\n",
        "        print(\"\\nDistribui√ß√£o das Top 5 categorias por tipo de incidente:\")\n",
        "        for cat in top_5_categories:\n",
        "            if cat in cross_tab.index:\n",
        "                print(f\"\\nCategoria {cat}:\")\n",
        "                for grade in cross_tab.columns:\n",
        "                    percentage = cross_tab.loc[cat, grade]\n",
        "                    grade_name = {0: 'FalsePositive', 1: 'TruePositive', 2: 'BenignPositive'}.get(grade, f'Grade{grade}')\n",
        "                    print(f\"  {grade_name}: {percentage:.2f}%\")\n",
        "    \n",
        "else:\n",
        "    print(\"Coluna 'category' n√£o encontrada no dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. An√°lise Temporal dos Incidentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise temporal dos incidentes\n",
        "if 'timestamp' in df.columns:\n",
        "    # Converter timestamp para datetime\n",
        "    df['datetime'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    \n",
        "    # Extrair componentes temporais\n",
        "    df['year'] = df['datetime'].dt.year\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    df['day'] = df['datetime'].dt.day\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "    df['day_name'] = df['datetime'].dt.day_name()\n",
        "    \n",
        "    print(\"=== AN√ÅLISE TEMPORAL DOS INCIDENTES ===\")\n",
        "    \n",
        "    # Per√≠odo dos dados\n",
        "    print(f\"Per√≠odo dos dados:\")\n",
        "    print(f\"  Data inicial: {df['datetime'].min()}\")\n",
        "    print(f\"  Data final: {df['datetime'].max()}\")\n",
        "    print(f\"  Total de dias: {(df['datetime'].max() - df['datetime'].min()).days}\")\n",
        "    \n",
        "    # An√°lise por ano\n",
        "    yearly_counts = df['year'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR ANO ===\")\n",
        "    for year, count in yearly_counts.items():\n",
        "        print(f\"  {year}: {count:,} incidentes\")\n",
        "    \n",
        "    # An√°lise por m√™s\n",
        "    monthly_counts = df['month'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR M√äS ===\")\n",
        "    month_names = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', \n",
        "                   'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']\n",
        "    for month, count in monthly_counts.items():\n",
        "        print(f\"  {month_names[month-1]} ({month:02d}): {count:,} incidentes\")\n",
        "    \n",
        "    # An√°lise por dia da semana\n",
        "    daily_counts = df['day_of_week'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR DIA DA SEMANA ===\")\n",
        "    day_names = ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo']\n",
        "    for day, count in daily_counts.items():\n",
        "        print(f\"  {day_names[day]}: {count:,} incidentes\")\n",
        "    \n",
        "    # An√°lise por hora\n",
        "    hourly_counts = df['hour'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR HORA DO DIA ===\")\n",
        "    print(\"Top 5 horas com mais incidentes:\")\n",
        "    for hour, count in hourly_counts.head(5).items():\n",
        "        print(f\"  {hour:02d}:00 - {hour:02d}:59: {count:,} incidentes\")\n",
        "    \n",
        "    # Visualiza√ß√µes temporais\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Gr√°fico 1: Incidentes por m√™s\n",
        "    axes[0, 0].bar(monthly_counts.index, monthly_counts.values, color='lightblue')\n",
        "    axes[0, 0].set_title('Incidentes por M√™s')\n",
        "    axes[0, 0].set_xlabel('M√™s')\n",
        "    axes[0, 0].set_ylabel('N√∫mero de Incidentes')\n",
        "    axes[0, 0].set_xticks(monthly_counts.index)\n",
        "    axes[0, 0].set_xticklabels([month_names[i-1] for i in monthly_counts.index], rotation=45)\n",
        "    \n",
        "    # Gr√°fico 2: Incidentes por dia da semana\n",
        "    axes[0, 1].bar(daily_counts.index, daily_counts.values, color='lightgreen')\n",
        "    axes[0, 1].set_title('Incidentes por Dia da Semana')\n",
        "    axes[0, 1].set_xlabel('Dia da Semana')\n",
        "    axes[0, 1].set_ylabel('N√∫mero de Incidentes')\n",
        "    axes[0, 1].set_xticks(daily_counts.index)\n",
        "    axes[0, 1].set_xticklabels([day_names[i] for i in daily_counts.index], rotation=45)\n",
        "    \n",
        "    # Gr√°fico 3: Incidentes por hora\n",
        "    axes[1, 0].plot(hourly_counts.index, hourly_counts.values, marker='o', color='red')\n",
        "    axes[1, 0].set_title('Incidentes por Hora do Dia')\n",
        "    axes[1, 0].set_xlabel('Hora')\n",
        "    axes[1, 0].set_ylabel('N√∫mero de Incidentes')\n",
        "    axes[1, 0].set_xticks(range(0, 24, 2))\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Gr√°fico 4: Timeline de incidentes\n",
        "    daily_incidents = df.groupby(df['datetime'].dt.date).size()\n",
        "    axes[1, 1].plot(daily_incidents.index, daily_incidents.values, alpha=0.7, color='purple')\n",
        "    axes[1, 1].set_title('Timeline de Incidentes (Di√°rio)')\n",
        "    axes[1, 1].set_xlabel('Data')\n",
        "    axes[1, 1].set_ylabel('N√∫mero de Incidentes')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"Coluna 'timestamp' n√£o encontrada no dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. An√°lise de Correla√ß√µes e Padr√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise de correla√ß√µes e padr√µes\n",
        "print(\"=== AN√ÅLISE DE CORRELA√á√ïES ===\")\n",
        "\n",
        "# Selecionar colunas num√©ricas para an√°lise de correla√ß√£o\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Colunas num√©ricas dispon√≠veis: {len(numeric_columns)}\")\n",
        "\n",
        "if len(numeric_columns) > 1:\n",
        "    # Calcular matriz de correla√ß√£o\n",
        "    correlation_matrix = df[numeric_columns].corr()\n",
        "    \n",
        "    # Encontrar correla√ß√µes mais altas (excluindo correla√ß√£o com si mesmo)\n",
        "    high_correlations = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            col1 = correlation_matrix.columns[i]\n",
        "            col2 = correlation_matrix.columns[j]\n",
        "            corr_value = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_value) > 0.5:  # Correla√ß√£o significativa\n",
        "                high_correlations.append((col1, col2, corr_value))\n",
        "    \n",
        "    # Ordenar por valor absoluto da correla√ß√£o\n",
        "    high_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "    \n",
        "    print(f\"\\nCorrela√ß√µes significativas (|r| > 0.5):\")\n",
        "    if high_correlations:\n",
        "        for col1, col2, corr in high_correlations[:10]:  # Top 10\n",
        "            print(f\"  {col1} ‚Üî {col2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma correla√ß√£o significativa encontrada.\")\n",
        "    \n",
        "    # Visualiza√ß√£o da matriz de correla√ß√£o\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # Criar heatmap de correla√ß√£o\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # M√°scara para mostrar apenas metade\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "    plt.title('Matriz de Correla√ß√£o das Vari√°veis Num√©ricas')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # An√°lise de correla√ß√£o com a vari√°vel target\n",
        "    if 'incidentgrade' in numeric_columns:\n",
        "        target_correlations = correlation_matrix['incidentgrade'].drop('incidentgrade').sort_values(key=abs, ascending=False)\n",
        "        \n",
        "        print(f\"\\n=== CORRELA√á√ïES COM A VARI√ÅVEL TARGET (IncidentGrade) ===\")\n",
        "        print(\"Top 10 vari√°veis mais correlacionadas com IncidentGrade:\")\n",
        "        for col, corr in target_correlations.head(10).items():\n",
        "            print(f\"  {col}: {corr:.3f}\")\n",
        "        \n",
        "        # Visualiza√ß√£o das correla√ß√µes com target\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        target_correlations_abs = target_correlations.abs()\n",
        "        top_15 = target_correlations_abs.head(15)\n",
        "        \n",
        "        plt.barh(range(len(top_15)), top_15.values, color='lightcoral')\n",
        "        plt.yticks(range(len(top_15)), top_15.index)\n",
        "        plt.xlabel('Correla√ß√£o Absoluta com IncidentGrade')\n",
        "        plt.title('Top 15 Vari√°veis com Maior Correla√ß√£o Absoluta com IncidentGrade')\n",
        "        plt.gca().invert_yaxis()\n",
        "        \n",
        "        # Adicionar valores nas barras\n",
        "        for i, (col, corr) in enumerate(zip(top_15.index, target_correlations[top_15.index])):\n",
        "            plt.text(corr + 0.01 if corr >= 0 else corr - 0.01, i, f'{corr:.3f}', \n",
        "                    va='center', ha='left' if corr >= 0 else 'right')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Poucas colunas num√©ricas dispon√≠veis para an√°lise de correla√ß√£o.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. An√°lise Geogr√°fica dos Incidentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lise geogr√°fica dos incidentes\n",
        "print(\"=== AN√ÅLISE GEOGR√ÅFICA DOS INCIDENTES ===\")\n",
        "\n",
        "# An√°lise por pa√≠s\n",
        "if 'countrycode' in df.columns:\n",
        "    country_counts = df['countrycode'].value_counts()\n",
        "    print(f\"Total de pa√≠ses √∫nicos: {len(country_counts)}\")\n",
        "    \n",
        "    print(\"\\nTop 10 pa√≠ses com mais incidentes:\")\n",
        "    for i, (country, count) in enumerate(country_counts.head(10).items()):\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {i+1:2d}. Pa√≠s {country}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # An√°lise por estado (se dispon√≠vel)\n",
        "    if 'state' in df.columns:\n",
        "        state_counts = df['state'].value_counts()\n",
        "        print(f\"\\nTotal de estados √∫nicos: {len(state_counts)}\")\n",
        "        \n",
        "        print(\"\\nTop 10 estados com mais incidentes:\")\n",
        "        for i, (state, count) in enumerate(state_counts.head(10).items()):\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"  {i+1:2d}. Estado {state}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # An√°lise por cidade (se dispon√≠vel)\n",
        "    if 'city' in df.columns:\n",
        "        city_counts = df['city'].value_counts()\n",
        "        print(f\"\\nTotal de cidades √∫nicas: {len(city_counts)}\")\n",
        "        \n",
        "        print(\"\\nTop 10 cidades com mais incidentes:\")\n",
        "        for i, (city, count) in enumerate(city_counts.head(10).items()):\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"  {i+1:2d}. Cidade {city}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualiza√ß√µes geogr√°ficas\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Gr√°fico 1: Top pa√≠ses\n",
        "    top_countries = country_counts.head(10)\n",
        "    axes[0, 0].barh(range(len(top_countries)), top_countries.values, color='lightblue')\n",
        "    axes[0, 0].set_yticks(range(len(top_countries)))\n",
        "    axes[0, 0].set_yticklabels([f'Pa√≠s {country}' for country in top_countries.index])\n",
        "    axes[0, 0].set_xlabel('N√∫mero de Incidentes')\n",
        "    axes[0, 0].set_title('Top 10 Pa√≠ses por N√∫mero de Incidentes')\n",
        "    axes[0, 0].invert_yaxis()\n",
        "    \n",
        "    # Gr√°fico 2: Top estados (se dispon√≠vel)\n",
        "    if 'state' in df.columns:\n",
        "        top_states = state_counts.head(10)\n",
        "        axes[0, 1].barh(range(len(top_states)), top_states.values, color='lightgreen')\n",
        "        axes[0, 1].set_yticks(range(len(top_states)))\n",
        "        axes[0, 1].set_yticklabels([f'Estado {state}' for state in top_states.index])\n",
        "        axes[0, 1].set_xlabel('N√∫mero de Incidentes')\n",
        "        axes[0, 1].set_title('Top 10 Estados por N√∫mero de Incidentes')\n",
        "        axes[0, 1].invert_yaxis()\n",
        "    else:\n",
        "        axes[0, 1].text(0.5, 0.5, 'Dados de estado n√£o dispon√≠veis', \n",
        "                        ha='center', va='center', transform=axes[0, 1].transAxes)\n",
        "        axes[0, 1].set_title('An√°lise por Estado')\n",
        "    \n",
        "    # Gr√°fico 3: Distribui√ß√£o geogr√°fica por tipo de incidente\n",
        "    if 'incidentgrade' in df.columns:\n",
        "        geo_incident = pd.crosstab(df['countrycode'], df['incidentgrade'])\n",
        "        top_5_countries = country_counts.head(5).index\n",
        "        \n",
        "        # Preparar dados para o gr√°fico\n",
        "        incident_types = geo_incident.columns\n",
        "        x = np.arange(len(top_5_countries))\n",
        "        width = 0.25\n",
        "        \n",
        "        for i, incident_type in enumerate(incident_types):\n",
        "            values = [geo_incident.loc[country, incident_type] if country in geo_incident.index else 0 \n",
        "                     for country in top_5_countries]\n",
        "            axes[1, 0].bar(x + i*width, values, width, \n",
        "                          label=f'Grade {incident_type}')\n",
        "        \n",
        "        axes[1, 0].set_xlabel('Pa√≠s')\n",
        "        axes[1, 0].set_ylabel('N√∫mero de Incidentes')\n",
        "        axes[1, 0].set_title('Distribui√ß√£o de Tipos de Incidente por Pa√≠s')\n",
        "        axes[1, 0].set_xticks(x + width)\n",
        "        axes[1, 0].set_xticklabels([f'Pa√≠s {country}' for country in top_5_countries])\n",
        "        axes[1, 0].legend()\n",
        "    \n",
        "    # Gr√°fico 4: Concentra√ß√£o de incidentes\n",
        "    if 'city' in df.columns:\n",
        "        top_cities = city_counts.head(10)\n",
        "        axes[1, 1].bar(range(len(top_cities)), top_cities.values, color='orange')\n",
        "        axes[1, 1].set_xticks(range(len(top_cities)))\n",
        "        axes[1, 1].set_xticklabels([f'Cidade {city}' for city in top_cities.index], rotation=45)\n",
        "        axes[1, 1].set_ylabel('N√∫mero de Incidentes')\n",
        "        axes[1, 1].set_title('Top 10 Cidades por N√∫mero de Incidentes')\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'Dados de cidade n√£o dispon√≠veis', \n",
        "                        ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].set_title('An√°lise por Cidade')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Coluna 'countrycode' n√£o encontrada no dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Resumo e Conclus√µes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo e conclus√µes da an√°lise\n",
        "print(\"=== RESUMO E CONCLUS√ïES DA AN√ÅLISE ===\")\n",
        "\n",
        "print(f\"\\nüìä ESTAT√çSTICAS GERAIS:\")\n",
        "print(f\"  ‚Ä¢ Total de registros: {len(df):,}\")\n",
        "print(f\"  ‚Ä¢ Total de colunas: {len(df.columns)}\")\n",
        "print(f\"  ‚Ä¢ Mem√≥ria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "if 'incidentgrade' in df.columns:\n",
        "    target_counts = df['incidentgrade'].value_counts()\n",
        "    print(f\"\\n VARI√ÅVEL TARGET (IncidentGrade):\")\n",
        "    print(f\"  ‚Ä¢ FalsePositive: {target_counts.get(0, 0):,} ({target_counts.get(0, 0)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ TruePositive: {target_counts.get(1, 0):,} ({target_counts.get(1, 0)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  ‚Ä¢ BenignPositive: {target_counts.get(2, 0):,} ({target_counts.get(2, 0)/len(df)*100:.1f}%)\")\n",
        "\n",
        "if 'category' in df.columns:\n",
        "    category_counts = df['category'].value_counts()\n",
        "    print(f\"\\n CATEGORIAS DE INCIDENTES:\")\n",
        "    print(f\"  ‚Ä¢ Total de categorias: {len(category_counts)}\")\n",
        "    print(f\"  ‚Ä¢ Categoria mais frequente: Categoria {category_counts.index[0]} ({category_counts.iloc[0]:,} incidentes)\")\n",
        "\n",
        "if 'timestamp' in df.columns:\n",
        "    df['datetime'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    print(f\"\\n AN√ÅLISE TEMPORAL:\")\n",
        "    print(f\"  ‚Ä¢ Per√≠odo: {df['datetime'].min()} a {df['datetime'].max()}\")\n",
        "    print(f\"  ‚Ä¢ Total de dias: {(df['datetime'].max() - df['datetime'].min()).days}\")\n",
        "    \n",
        "    # Hora com mais incidentes\n",
        "    hourly_counts = df['hour'].value_counts().sort_index()\n",
        "    peak_hour = hourly_counts.idxmax()\n",
        "    print(f\"  ‚Ä¢ Hora de pico: {peak_hour:02d}:00 ({hourly_counts.max():,} incidentes)\")\n",
        "\n",
        "if 'countrycode' in df.columns:\n",
        "    country_counts = df['countrycode'].value_counts()\n",
        "    print(f\"\\n AN√ÅLISE GEOGR√ÅFICA:\")\n",
        "    print(f\"  ‚Ä¢ Total de pa√≠ses: {len(country_counts)}\")\n",
        "    print(f\"  ‚Ä¢ Pa√≠s com mais incidentes: Pa√≠s {country_counts.index[0]} ({country_counts.iloc[0]:,} incidentes)\")\n",
        "\n",
        "print(f\"\\n QUALIDADE DOS DADOS:\")\n",
        "print(f\"  ‚Ä¢ Valores ausentes: {df.isnull().sum().sum()}\")\n",
        "print(f\"  ‚Ä¢ Valores infinitos: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "print(f\"  ‚Ä¢ Tipos de dados: {df.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "print(f\"\\n PRINCIPAIS INSIGHTS:\")\n",
        "print(f\"  1. Dataset com alta qualidade ap√≥s processamento ETL\")\n",
        "print(f\"  2. Vari√°vel target com distribui√ß√£o desbalanceada\")\n",
        "print(f\"  3. M√∫ltiplas categorias de incidentes identificadas\")\n",
        "print(f\"  4. Padr√µes temporais e geogr√°ficos claros\")\n",
        "print(f\"  5. Dados prontos para modelagem de Machine Learning\")\n",
        "\n",
        "print(f\"\\n PR√ìXIMOS PASSOS RECOMENDADOS:\")\n",
        "print(f\"  1. Balanceamento de classes para melhor performance do modelo\")\n",
        "print(f\"  2. Feature engineering para capturar padr√µes temporais\")\n",
        "print(f\"  3. Sele√ß√£o de features baseada em correla√ß√µes\")\n",
        "print(f\"  4. Aplica√ß√£o de algoritmos de classifica√ß√£o\")\n",
        "print(f\"  5. Valida√ß√£o cruzada e otimiza√ß√£o de hiperpar√¢metros\")\n",
        "\n",
        "print(f\"\\n APLICA√á√ïES POTENCIAIS:\")\n",
        "print(f\"  ‚Ä¢ Sistema de detec√ß√£o autom√°tica de incidentes\")\n",
        "print(f\"  ‚Ä¢ Classifica√ß√£o de amea√ßas em tempo real\")\n",
        "print(f\"  ‚Ä¢ An√°lise preditiva de riscos de seguran√ßa\")\n",
        "print(f\"  ‚Ä¢ Dashboard de monitoramento de seguran√ßa\")\n",
        "print(f\"  ‚Ä¢ Sistema de alertas inteligentes\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\" AN√ÅLISE CONCLU√çDA COM SUCESSO!\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
