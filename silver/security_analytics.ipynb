{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Análise de Dados - Microsoft Security Incident Prediction (Camada Silver)\n",
        "\n",
        "Este notebook realiza análises exploratórias e estatísticas dos dados processados da camada Silver do dataset Microsoft Security Incident Prediction.\n",
        "\n",
        "## Objetivos da Análise\n",
        "- Compreender a distribuição dos dados\n",
        "- Identificar padrões e tendências\n",
        "- Analisar a qualidade dos dados processados\n",
        "- Preparar insights para modelagem de Machine Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importação das Bibliotecas e Carregamento dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurações para visualização\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Carregamento dos dados processados da camada Silver\n",
        "df = pd.read_csv('security_incident_prediction_silver.csv', low_memory=False)\n",
        "\n",
        "print(\"Dataset carregado com sucesso!\")\n",
        "print(f\"Dimensões do dataset: {df.shape}\")\n",
        "print(f\"Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Visualizar as primeiras linhas\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Análise Geral dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informações gerais sobre o dataset\n",
        "print(\"=== INFORMAÇÕES GERAIS DO DATASET ===\")\n",
        "print(f\"Número de registros: {len(df):,}\")\n",
        "print(f\"Número de colunas: {len(df.columns)}\")\n",
        "print(f\"Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "print(\"\\n=== TIPOS DE DADOS ===\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(\"\\n=== INFORMAÇÕES DETALHADAS ===\")\n",
        "df.info()\n",
        "\n",
        "print(\"\\n=== ESTATÍSTICAS DESCRITIVAS ===\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Análise da Variável Target (IncidentGrade)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise da variável target IncidentGrade\n",
        "if 'incidentgrade' in df.columns:\n",
        "    target_counts = df['incidentgrade'].value_counts()\n",
        "    target_percentages = df['incidentgrade'].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(\"=== DISTRIBUIÇÃO DA VARIÁVEL TARGET (IncidentGrade) ===\")\n",
        "    print(\"\\nContagem por classe:\")\n",
        "    for value, count in target_counts.items():\n",
        "        print(f\"  Classe {value}: {count:,} ({target_percentages[value]:.2f}%)\")\n",
        "    \n",
        "    # Mapear os valores para nomes mais legíveis (baseado no dataset original)\n",
        "    class_mapping = {0: 'FalsePositive', 1: 'TruePositive', 2: 'BenignPositive'}\n",
        "    print(\"\\nMapeamento das classes:\")\n",
        "    for encoded_val, original_val in class_mapping.items():\n",
        "        if encoded_val in target_counts.index:\n",
        "            count = target_counts[encoded_val]\n",
        "            percentage = target_percentages[encoded_val]\n",
        "            print(f\"  {encoded_val} = {original_val}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualização da distribuição\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # Subplot 1: Gráfico de barras\n",
        "    plt.subplot(1, 2, 1)\n",
        "    bars = plt.bar(target_counts.index, target_counts.values, color=['#ff7f7f', '#7fbf7f', '#7f7fff'])\n",
        "    plt.title('Distribuição da Variável Target (IncidentGrade)')\n",
        "    plt.xlabel('Classe')\n",
        "    plt.ylabel('Contagem')\n",
        "    \n",
        "    # Adicionar percentuais nas barras\n",
        "    for i, (bar, count, pct) in enumerate(zip(bars, target_counts.values, target_percentages.values)):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + bar.get_height()*0.01,\n",
        "                f'{count:,}\\n({pct:.1f}%)', ha='center', va='bottom')\n",
        "    \n",
        "    # Subplot 2: Gráfico de pizza\n",
        "    plt.subplot(1, 2, 2)\n",
        "    labels = [f'{class_mapping.get(i, f\"Classe {i}\")}\\n({pct:.1f}%)' \n",
        "              for i, pct in zip(target_counts.index, target_percentages.values)]\n",
        "    plt.pie(target_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "    plt.title('Distribuição Percentual das Classes')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Análise de balanceamento\n",
        "    balance_ratio = min(target_counts) / max(target_counts)\n",
        "    print(f\"\\nRazão de balanceamento: {balance_ratio:.3f}\")\n",
        "    if balance_ratio < 0.1:\n",
        "        print(\"  Dataset altamente desbalanceado!\")\n",
        "    elif balance_ratio < 0.3:\n",
        "        print(\"  Dataset moderadamente desbalanceado\")\n",
        "    else:\n",
        "        print(\" Dataset relativamente balanceado\")\n",
        "        \n",
        "else:\n",
        "    print(\"Variável 'incidentgrade' não encontrada no dataset.\")\n",
        "    print(\"Colunas disponíveis:\", df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Análise de Categorias de Incidentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise das categorias de incidentes\n",
        "if 'category' in df.columns:\n",
        "    category_counts = df['category'].value_counts()\n",
        "    category_percentages = df['category'].value_counts(normalize=True) * 100\n",
        "    \n",
        "    print(\"=== ANÁLISE DE CATEGORIAS DE INCIDENTES ===\")\n",
        "    print(f\"Total de categorias únicas: {len(category_counts)}\")\n",
        "    \n",
        "    print(\"\\nTop 10 categorias mais frequentes:\")\n",
        "    for i, (category, count) in enumerate(category_counts.head(10).items()):\n",
        "        percentage = category_percentages[category]\n",
        "        print(f\"  {i+1:2d}. Categoria {category}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualização das categorias\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    \n",
        "    # Gráfico de barras horizontais para as top categorias\n",
        "    top_categories = category_counts.head(15)\n",
        "    plt.barh(range(len(top_categories)), top_categories.values, color='skyblue')\n",
        "    plt.yticks(range(len(top_categories)), [f'Categoria {cat}' for cat in top_categories.index])\n",
        "    plt.xlabel('Número de Incidentes')\n",
        "    plt.title('Top 15 Categorias de Incidentes por Frequência')\n",
        "    plt.gca().invert_yaxis()\n",
        "    \n",
        "    # Adicionar valores nas barras\n",
        "    for i, (cat, count) in enumerate(top_categories.items()):\n",
        "        plt.text(count + count*0.01, i, f'{count:,}', va='center', ha='left')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Análise de distribuição por categoria e incident grade\n",
        "    if 'incidentgrade' in df.columns:\n",
        "        print(\"\\n=== DISTRIBUIÇÃO DE CATEGORIAS POR TIPO DE INCIDENTE ===\")\n",
        "        cross_tab = pd.crosstab(df['category'], df['incidentgrade'], normalize='index') * 100\n",
        "        \n",
        "        # Mostrar as top 5 categorias\n",
        "        top_5_categories = category_counts.head(5).index\n",
        "        print(\"\\nDistribuição das Top 5 categorias por tipo de incidente:\")\n",
        "        for cat in top_5_categories:\n",
        "            if cat in cross_tab.index:\n",
        "                print(f\"\\nCategoria {cat}:\")\n",
        "                for grade in cross_tab.columns:\n",
        "                    percentage = cross_tab.loc[cat, grade]\n",
        "                    grade_name = {0: 'FalsePositive', 1: 'TruePositive', 2: 'BenignPositive'}.get(grade, f'Grade{grade}')\n",
        "                    print(f\"  {grade_name}: {percentage:.2f}%\")\n",
        "    \n",
        "else:\n",
        "    print(\"Coluna 'category' não encontrada no dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Análise Temporal dos Incidentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise temporal dos incidentes\n",
        "if 'timestamp' in df.columns:\n",
        "    # Converter timestamp para datetime\n",
        "    df['datetime'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    \n",
        "    # Extrair componentes temporais\n",
        "    df['year'] = df['datetime'].dt.year\n",
        "    df['month'] = df['datetime'].dt.month\n",
        "    df['day'] = df['datetime'].dt.day\n",
        "    df['hour'] = df['datetime'].dt.hour\n",
        "    df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "    df['day_name'] = df['datetime'].dt.day_name()\n",
        "    \n",
        "    print(\"=== ANÁLISE TEMPORAL DOS INCIDENTES ===\")\n",
        "    \n",
        "    # Período dos dados\n",
        "    print(f\"Período dos dados:\")\n",
        "    print(f\"  Data inicial: {df['datetime'].min()}\")\n",
        "    print(f\"  Data final: {df['datetime'].max()}\")\n",
        "    print(f\"  Total de dias: {(df['datetime'].max() - df['datetime'].min()).days}\")\n",
        "    \n",
        "    # Análise por ano\n",
        "    yearly_counts = df['year'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR ANO ===\")\n",
        "    for year, count in yearly_counts.items():\n",
        "        print(f\"  {year}: {count:,} incidentes\")\n",
        "    \n",
        "    # Análise por mês\n",
        "    monthly_counts = df['month'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR MÊS ===\")\n",
        "    month_names = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun', \n",
        "                   'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']\n",
        "    for month, count in monthly_counts.items():\n",
        "        print(f\"  {month_names[month-1]} ({month:02d}): {count:,} incidentes\")\n",
        "    \n",
        "    # Análise por dia da semana\n",
        "    daily_counts = df['day_of_week'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR DIA DA SEMANA ===\")\n",
        "    day_names = ['Segunda', 'Terça', 'Quarta', 'Quinta', 'Sexta', 'Sábado', 'Domingo']\n",
        "    for day, count in daily_counts.items():\n",
        "        print(f\"  {day_names[day]}: {count:,} incidentes\")\n",
        "    \n",
        "    # Análise por hora\n",
        "    hourly_counts = df['hour'].value_counts().sort_index()\n",
        "    print(f\"\\n=== INCIDENTES POR HORA DO DIA ===\")\n",
        "    print(\"Top 5 horas com mais incidentes:\")\n",
        "    for hour, count in hourly_counts.head(5).items():\n",
        "        print(f\"  {hour:02d}:00 - {hour:02d}:59: {count:,} incidentes\")\n",
        "    \n",
        "    # Visualizações temporais\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Gráfico 1: Incidentes por mês\n",
        "    axes[0, 0].bar(monthly_counts.index, monthly_counts.values, color='lightblue')\n",
        "    axes[0, 0].set_title('Incidentes por Mês')\n",
        "    axes[0, 0].set_xlabel('Mês')\n",
        "    axes[0, 0].set_ylabel('Número de Incidentes')\n",
        "    axes[0, 0].set_xticks(monthly_counts.index)\n",
        "    axes[0, 0].set_xticklabels([month_names[i-1] for i in monthly_counts.index], rotation=45)\n",
        "    \n",
        "    # Gráfico 2: Incidentes por dia da semana\n",
        "    axes[0, 1].bar(daily_counts.index, daily_counts.values, color='lightgreen')\n",
        "    axes[0, 1].set_title('Incidentes por Dia da Semana')\n",
        "    axes[0, 1].set_xlabel('Dia da Semana')\n",
        "    axes[0, 1].set_ylabel('Número de Incidentes')\n",
        "    axes[0, 1].set_xticks(daily_counts.index)\n",
        "    axes[0, 1].set_xticklabels([day_names[i] for i in daily_counts.index], rotation=45)\n",
        "    \n",
        "    # Gráfico 3: Incidentes por hora\n",
        "    axes[1, 0].plot(hourly_counts.index, hourly_counts.values, marker='o', color='red')\n",
        "    axes[1, 0].set_title('Incidentes por Hora do Dia')\n",
        "    axes[1, 0].set_xlabel('Hora')\n",
        "    axes[1, 0].set_ylabel('Número de Incidentes')\n",
        "    axes[1, 0].set_xticks(range(0, 24, 2))\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Gráfico 4: Timeline de incidentes\n",
        "    daily_incidents = df.groupby(df['datetime'].dt.date).size()\n",
        "    axes[1, 1].plot(daily_incidents.index, daily_incidents.values, alpha=0.7, color='purple')\n",
        "    axes[1, 1].set_title('Timeline de Incidentes (Diário)')\n",
        "    axes[1, 1].set_xlabel('Data')\n",
        "    axes[1, 1].set_ylabel('Número de Incidentes')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"Coluna 'timestamp' não encontrada no dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Análise de Correlações e Padrões\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise de correlações e padrões\n",
        "print(\"=== ANÁLISE DE CORRELAÇÕES ===\")\n",
        "\n",
        "# Selecionar colunas numéricas para análise de correlação\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Colunas numéricas disponíveis: {len(numeric_columns)}\")\n",
        "\n",
        "if len(numeric_columns) > 1:\n",
        "    # Calcular matriz de correlação\n",
        "    correlation_matrix = df[numeric_columns].corr()\n",
        "    \n",
        "    # Encontrar correlações mais altas (excluindo correlação com si mesmo)\n",
        "    high_correlations = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            col1 = correlation_matrix.columns[i]\n",
        "            col2 = correlation_matrix.columns[j]\n",
        "            corr_value = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_value) > 0.5:  # Correlação significativa\n",
        "                high_correlations.append((col1, col2, corr_value))\n",
        "    \n",
        "    # Ordenar por valor absoluto da correlação\n",
        "    high_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "    \n",
        "    print(f\"\\nCorrelações significativas (|r| > 0.5):\")\n",
        "    if high_correlations:\n",
        "        for col1, col2, corr in high_correlations[:10]:  # Top 10\n",
        "            print(f\"  {col1} ↔ {col2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(\"  Nenhuma correlação significativa encontrada.\")\n",
        "    \n",
        "    # Visualização da matriz de correlação\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    \n",
        "    # Criar heatmap de correlação\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Máscara para mostrar apenas metade\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
        "    plt.title('Matriz de Correlação das Variáveis Numéricas')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Análise de correlação com a variável target\n",
        "    if 'incidentgrade' in numeric_columns:\n",
        "        target_correlations = correlation_matrix['incidentgrade'].drop('incidentgrade').sort_values(key=abs, ascending=False)\n",
        "        \n",
        "        print(f\"\\n=== CORRELAÇÕES COM A VARIÁVEL TARGET (IncidentGrade) ===\")\n",
        "        print(\"Top 10 variáveis mais correlacionadas com IncidentGrade:\")\n",
        "        for col, corr in target_correlations.head(10).items():\n",
        "            print(f\"  {col}: {corr:.3f}\")\n",
        "        \n",
        "        # Visualização das correlações com target\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        target_correlations_abs = target_correlations.abs()\n",
        "        top_15 = target_correlations_abs.head(15)\n",
        "        \n",
        "        plt.barh(range(len(top_15)), top_15.values, color='lightcoral')\n",
        "        plt.yticks(range(len(top_15)), top_15.index)\n",
        "        plt.xlabel('Correlação Absoluta com IncidentGrade')\n",
        "        plt.title('Top 15 Variáveis com Maior Correlação Absoluta com IncidentGrade')\n",
        "        plt.gca().invert_yaxis()\n",
        "        \n",
        "        # Adicionar valores nas barras\n",
        "        for i, (col, corr) in enumerate(zip(top_15.index, target_correlations[top_15.index])):\n",
        "            plt.text(corr + 0.01 if corr >= 0 else corr - 0.01, i, f'{corr:.3f}', \n",
        "                    va='center', ha='left' if corr >= 0 else 'right')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Poucas colunas numéricas disponíveis para análise de correlação.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Análise Geográfica dos Incidentes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Análise geográfica dos incidentes\n",
        "print(\"=== ANÁLISE GEOGRÁFICA DOS INCIDENTES ===\")\n",
        "\n",
        "# Análise por país\n",
        "if 'countrycode' in df.columns:\n",
        "    country_counts = df['countrycode'].value_counts()\n",
        "    print(f\"Total de países únicos: {len(country_counts)}\")\n",
        "    \n",
        "    print(\"\\nTop 10 países com mais incidentes:\")\n",
        "    for i, (country, count) in enumerate(country_counts.head(10).items()):\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {i+1:2d}. País {country}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Análise por estado (se disponível)\n",
        "    if 'state' in df.columns:\n",
        "        state_counts = df['state'].value_counts()\n",
        "        print(f\"\\nTotal de estados únicos: {len(state_counts)}\")\n",
        "        \n",
        "        print(\"\\nTop 10 estados com mais incidentes:\")\n",
        "        for i, (state, count) in enumerate(state_counts.head(10).items()):\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"  {i+1:2d}. Estado {state}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Análise por cidade (se disponível)\n",
        "    if 'city' in df.columns:\n",
        "        city_counts = df['city'].value_counts()\n",
        "        print(f\"\\nTotal de cidades únicas: {len(city_counts)}\")\n",
        "        \n",
        "        print(\"\\nTop 10 cidades com mais incidentes:\")\n",
        "        for i, (city, count) in enumerate(city_counts.head(10).items()):\n",
        "            percentage = (count / len(df)) * 100\n",
        "            print(f\"  {i+1:2d}. Cidade {city}: {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Visualizações geográficas\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "    \n",
        "    # Gráfico 1: Top países\n",
        "    top_countries = country_counts.head(10)\n",
        "    axes[0, 0].barh(range(len(top_countries)), top_countries.values, color='lightblue')\n",
        "    axes[0, 0].set_yticks(range(len(top_countries)))\n",
        "    axes[0, 0].set_yticklabels([f'País {country}' for country in top_countries.index])\n",
        "    axes[0, 0].set_xlabel('Número de Incidentes')\n",
        "    axes[0, 0].set_title('Top 10 Países por Número de Incidentes')\n",
        "    axes[0, 0].invert_yaxis()\n",
        "    \n",
        "    # Gráfico 2: Top estados (se disponível)\n",
        "    if 'state' in df.columns:\n",
        "        top_states = state_counts.head(10)\n",
        "        axes[0, 1].barh(range(len(top_states)), top_states.values, color='lightgreen')\n",
        "        axes[0, 1].set_yticks(range(len(top_states)))\n",
        "        axes[0, 1].set_yticklabels([f'Estado {state}' for state in top_states.index])\n",
        "        axes[0, 1].set_xlabel('Número de Incidentes')\n",
        "        axes[0, 1].set_title('Top 10 Estados por Número de Incidentes')\n",
        "        axes[0, 1].invert_yaxis()\n",
        "    else:\n",
        "        axes[0, 1].text(0.5, 0.5, 'Dados de estado não disponíveis', \n",
        "                        ha='center', va='center', transform=axes[0, 1].transAxes)\n",
        "        axes[0, 1].set_title('Análise por Estado')\n",
        "    \n",
        "    # Gráfico 3: Distribuição geográfica por tipo de incidente\n",
        "    if 'incidentgrade' in df.columns:\n",
        "        geo_incident = pd.crosstab(df['countrycode'], df['incidentgrade'])\n",
        "        top_5_countries = country_counts.head(5).index\n",
        "        \n",
        "        # Preparar dados para o gráfico\n",
        "        incident_types = geo_incident.columns\n",
        "        x = np.arange(len(top_5_countries))\n",
        "        width = 0.25\n",
        "        \n",
        "        for i, incident_type in enumerate(incident_types):\n",
        "            values = [geo_incident.loc[country, incident_type] if country in geo_incident.index else 0 \n",
        "                     for country in top_5_countries]\n",
        "            axes[1, 0].bar(x + i*width, values, width, \n",
        "                          label=f'Grade {incident_type}')\n",
        "        \n",
        "        axes[1, 0].set_xlabel('País')\n",
        "        axes[1, 0].set_ylabel('Número de Incidentes')\n",
        "        axes[1, 0].set_title('Distribuição de Tipos de Incidente por País')\n",
        "        axes[1, 0].set_xticks(x + width)\n",
        "        axes[1, 0].set_xticklabels([f'País {country}' for country in top_5_countries])\n",
        "        axes[1, 0].legend()\n",
        "    \n",
        "    # Gráfico 4: Concentração de incidentes\n",
        "    if 'city' in df.columns:\n",
        "        top_cities = city_counts.head(10)\n",
        "        axes[1, 1].bar(range(len(top_cities)), top_cities.values, color='orange')\n",
        "        axes[1, 1].set_xticks(range(len(top_cities)))\n",
        "        axes[1, 1].set_xticklabels([f'Cidade {city}' for city in top_cities.index], rotation=45)\n",
        "        axes[1, 1].set_ylabel('Número de Incidentes')\n",
        "        axes[1, 1].set_title('Top 10 Cidades por Número de Incidentes')\n",
        "    else:\n",
        "        axes[1, 1].text(0.5, 0.5, 'Dados de cidade não disponíveis', \n",
        "                        ha='center', va='center', transform=axes[1, 1].transAxes)\n",
        "        axes[1, 1].set_title('Análise por Cidade')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"Coluna 'countrycode' não encontrada no dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Resumo e Conclusões\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resumo e conclusões da análise\n",
        "print(\"=== RESUMO E CONCLUSÕES DA ANÁLISE ===\")\n",
        "\n",
        "print(f\"\\n📊 ESTATÍSTICAS GERAIS:\")\n",
        "print(f\"  • Total de registros: {len(df):,}\")\n",
        "print(f\"  • Total de colunas: {len(df.columns)}\")\n",
        "print(f\"  • Memória utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "if 'incidentgrade' in df.columns:\n",
        "    target_counts = df['incidentgrade'].value_counts()\n",
        "    print(f\"\\n VARIÁVEL TARGET (IncidentGrade):\")\n",
        "    print(f\"  • FalsePositive: {target_counts.get(0, 0):,} ({target_counts.get(0, 0)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  • TruePositive: {target_counts.get(1, 0):,} ({target_counts.get(1, 0)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  • BenignPositive: {target_counts.get(2, 0):,} ({target_counts.get(2, 0)/len(df)*100:.1f}%)\")\n",
        "\n",
        "if 'category' in df.columns:\n",
        "    category_counts = df['category'].value_counts()\n",
        "    print(f\"\\n CATEGORIAS DE INCIDENTES:\")\n",
        "    print(f\"  • Total de categorias: {len(category_counts)}\")\n",
        "    print(f\"  • Categoria mais frequente: Categoria {category_counts.index[0]} ({category_counts.iloc[0]:,} incidentes)\")\n",
        "\n",
        "if 'timestamp' in df.columns:\n",
        "    df['datetime'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    print(f\"\\n ANÁLISE TEMPORAL:\")\n",
        "    print(f\"  • Período: {df['datetime'].min()} a {df['datetime'].max()}\")\n",
        "    print(f\"  • Total de dias: {(df['datetime'].max() - df['datetime'].min()).days}\")\n",
        "    \n",
        "    # Hora com mais incidentes\n",
        "    hourly_counts = df['hour'].value_counts().sort_index()\n",
        "    peak_hour = hourly_counts.idxmax()\n",
        "    print(f\"  • Hora de pico: {peak_hour:02d}:00 ({hourly_counts.max():,} incidentes)\")\n",
        "\n",
        "if 'countrycode' in df.columns:\n",
        "    country_counts = df['countrycode'].value_counts()\n",
        "    print(f\"\\n ANÁLISE GEOGRÁFICA:\")\n",
        "    print(f\"  • Total de países: {len(country_counts)}\")\n",
        "    print(f\"  • País com mais incidentes: País {country_counts.index[0]} ({country_counts.iloc[0]:,} incidentes)\")\n",
        "\n",
        "print(f\"\\n QUALIDADE DOS DADOS:\")\n",
        "print(f\"  • Valores ausentes: {df.isnull().sum().sum()}\")\n",
        "print(f\"  • Valores infinitos: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "print(f\"  • Tipos de dados: {df.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "print(f\"\\n PRINCIPAIS INSIGHTS:\")\n",
        "print(f\"  1. Dataset com alta qualidade após processamento ETL\")\n",
        "print(f\"  2. Variável target com distribuição desbalanceada\")\n",
        "print(f\"  3. Múltiplas categorias de incidentes identificadas\")\n",
        "print(f\"  4. Padrões temporais e geográficos claros\")\n",
        "print(f\"  5. Dados prontos para modelagem de Machine Learning\")\n",
        "\n",
        "print(f\"\\n PRÓXIMOS PASSOS RECOMENDADOS:\")\n",
        "print(f\"  1. Balanceamento de classes para melhor performance do modelo\")\n",
        "print(f\"  2. Feature engineering para capturar padrões temporais\")\n",
        "print(f\"  3. Seleção de features baseada em correlações\")\n",
        "print(f\"  4. Aplicação de algoritmos de classificação\")\n",
        "print(f\"  5. Validação cruzada e otimização de hiperparâmetros\")\n",
        "\n",
        "print(f\"\\n APLICAÇÕES POTENCIAIS:\")\n",
        "print(f\"  • Sistema de detecção automática de incidentes\")\n",
        "print(f\"  • Classificação de ameaças em tempo real\")\n",
        "print(f\"  • Análise preditiva de riscos de segurança\")\n",
        "print(f\"  • Dashboard de monitoramento de segurança\")\n",
        "print(f\"  • Sistema de alertas inteligentes\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\" ANÁLISE CONCLUÍDA COM SUCESSO!\")\n",
        "print(f\"{'='*60}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
